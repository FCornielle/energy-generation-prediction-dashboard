{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing plant: parque_solar_girasol\n",
      "Saved cleaned data for parque_solar_girasol to ..\\data\\interim\\meteo_data_with_generation\\processed_models\\parque_solar_girasol_clean.parquet\n",
      "parque_solar_girasol - LinearRegression: MSE = 117.56, R2 = 0.88\n",
      "parque_solar_girasol - RandomForest (tuned): MSE = 99.95, R2 = 0.90\n",
      "parque_solar_girasol - GradientBoosting (tuned): MSE = 98.48, R2 = 0.90\n",
      "Best model for parque_solar_girasol is GradientBoosting (R2 = 0.90) and has been saved to ..\\data\\interim\\meteo_data_with_generation\\processed_models\\models\\parque_solar_girasol_best_model.h5\n",
      "\n",
      "Overall model performance results:\n",
      "                                                       LinearRegression  \\\n",
      "parque_solar_girasol  {'MSE': 117.56472067441558, 'R2': 0.8802702746...   \n",
      "\n",
      "                                                           RandomForest  \\\n",
      "parque_solar_girasol  {'MSE': 99.95428011029138, 'R2': 0.89820501905...   \n",
      "\n",
      "                                                       GradientBoosting  \n",
      "parque_solar_girasol  {'MSE': 98.48286438999915, 'R2': 0.89970353152...  \n",
      "Saved performance metrics to ..\\data\\interim\\meteo_data_with_generation\\processed_models\\model_performance_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# =============================================\n",
    "# Step 1. Load all merged Parquet files\n",
    "# =============================================\n",
    "def load_all_merged_data(meteo_dir: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Loads all Parquet files from the given directory into a dictionary,\n",
    "    using the file stem (plant name) as the key.\n",
    "    Assumes each file contains a UTC-aware DatetimeIndex and a 'generation' column,\n",
    "    along with meteorological variables.\n",
    "    \"\"\"\n",
    "    plant_dfs = {}\n",
    "    for file in meteo_dir.glob(\"*.parquet\"):\n",
    "        df = pd.read_parquet(file)\n",
    "        # Ensure the index is a UTC-aware DatetimeIndex\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], utc=True)\n",
    "            df = df.set_index(\"date\")\n",
    "        plant_name = file.stem  # e.g. \"parque_eolico_agua_clara\"\n",
    "        plant_dfs[plant_name] = df\n",
    "    return plant_dfs\n",
    "\n",
    "# =============================================\n",
    "# Step 2. Preprocess each DataFrame\n",
    "#   a. Drop only the initial consecutive days with zero generation.\n",
    "#   b. Drop the last 26 hours.\n",
    "# =============================================\n",
    "def drop_initial_zero_generation_days(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops the very first consecutive days where the total generation is zero.\n",
    "    Once a day with non-zero generation is encountered, subsequent days are retained.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"date_only\"] = df.index.normalize()\n",
    "    unique_dates = sorted(df[\"date_only\"].unique())\n",
    "    drop_dates = []\n",
    "    for date in unique_dates:\n",
    "        daily_sum = df.loc[df[\"date_only\"] == date, \"generation\"].sum()\n",
    "        if daily_sum == 0:\n",
    "            drop_dates.append(date)\n",
    "        else:\n",
    "            break  # Stop after the first day with non-zero generation\n",
    "    df_clean = df[~df[\"date_only\"].isin(drop_dates)].copy()\n",
    "    df_clean.drop(columns=[\"date_only\"], inplace=True)\n",
    "    return df_clean\n",
    "\n",
    "def drop_last_26_hours(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops rows from the DataFrame where the timestamp is within the last 26 hours.\n",
    "    \"\"\"\n",
    "    last_timestamp = df.index.max()\n",
    "    cutoff = last_timestamp - timedelta(hours=26)\n",
    "    return df[df.index <= cutoff]\n",
    "\n",
    "# =============================================\n",
    "# Step 3. Grid Search Hyperparameter Tuning for Models\n",
    "# =============================================\n",
    "def tune_model_grid(model, param_grid, X_train, y_train, cv=3):\n",
    "    \"\"\"\n",
    "    Performs grid search using GridSearchCV on the given model.\n",
    "    Returns the best estimator.\n",
    "    \"\"\"\n",
    "    grid_cv = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=\"r2\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_cv.fit(X_train, y_train)\n",
    "    return grid_cv.best_estimator_\n",
    "\n",
    "def tune_and_evaluate_models(df: pd.DataFrame, plant: str) -> (dict, dict):\n",
    "    \"\"\"\n",
    "    Given a preprocessed DataFrame for a plant, this function:\n",
    "      - Drops rows with missing values.\n",
    "      - Splits data into training (first 80%) and testing (last 20%) sets (time-ordered).\n",
    "      - Uses all numeric meteorological variables (excluding 'generation') as features.\n",
    "      - Trains three models:\n",
    "            * Linear Regression (no tuning)\n",
    "            * Random Forest (with GridSearchCV)\n",
    "            * Gradient Boosting (with GridSearchCV)\n",
    "      - Evaluates each model using Mean Squared Error and R².\n",
    "    \n",
    "    Returns two dictionaries:\n",
    "      - performance: mapping model names to metrics.\n",
    "      - model_objs: mapping model names to the trained model objects.\n",
    "    \"\"\"\n",
    "    df = df.dropna()\n",
    "    if df.empty:\n",
    "        print(f\"No data available for {plant} after dropping missing values.\")\n",
    "        return {}, {}\n",
    "    \n",
    "    X = df.drop(columns=[\"generation\"])\n",
    "    y = df[\"generation\"]\n",
    "    X = X.select_dtypes(include=[np.number])\n",
    "    \n",
    "    split_index = int(len(df) * 0.8)\n",
    "    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "    \n",
    "    performance = {}\n",
    "    model_objs = {}\n",
    "    \n",
    "    # Linear Regression (no tuning)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "    r2_lr = r2_score(y_test, y_pred_lr)\n",
    "    performance[\"LinearRegression\"] = {\"MSE\": mse_lr, \"R2\": r2_lr}\n",
    "    model_objs[\"LinearRegression\"] = lr\n",
    "    print(f\"{plant} - LinearRegression: MSE = {mse_lr:.2f}, R2 = {r2_lr:.2f}\")\n",
    "    \n",
    "    # Random Forest with GridSearchCV\n",
    "    rf_param_grid = {\n",
    "        \"n_estimators\": [50, 100],\n",
    "        \"max_depth\": [5, 10],\n",
    "        \"min_samples_split\": [2, 4],\n",
    "        \"min_samples_leaf\": [1, 2]\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_best = tune_model_grid(rf, rf_param_grid, X_train, y_train, cv=3)\n",
    "    y_pred_rf = rf_best.predict(X_test)\n",
    "    mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "    r2_rf = r2_score(y_test, y_pred_rf)\n",
    "    performance[\"RandomForest\"] = {\"MSE\": mse_rf, \"R2\": r2_rf}\n",
    "    model_objs[\"RandomForest\"] = rf_best\n",
    "    print(f\"{plant} - RandomForest (tuned): MSE = {mse_rf:.2f}, R2 = {r2_rf:.2f}\")\n",
    "    \n",
    "    # Gradient Boosting with GridSearchCV\n",
    "    gb_param_grid = {\n",
    "        \"n_estimators\": [50, 100],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"max_depth\": [3, 5],\n",
    "        \"min_samples_split\": [2, 4],\n",
    "        \"min_samples_leaf\": [1, 2]\n",
    "    }\n",
    "    gb = GradientBoostingRegressor(random_state=42)\n",
    "    gb_best = tune_model_grid(gb, gb_param_grid, X_train, y_train, cv=3)\n",
    "    y_pred_gb = gb_best.predict(X_test)\n",
    "    mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "    r2_gb = r2_score(y_test, y_pred_gb)\n",
    "    performance[\"GradientBoosting\"] = {\"MSE\": mse_gb, \"R2\": r2_gb}\n",
    "    model_objs[\"GradientBoosting\"] = gb_best\n",
    "    print(f\"{plant} - GradientBoosting (tuned): MSE = {mse_gb:.2f}, R2 = {r2_gb:.2f}\")\n",
    "    \n",
    "    return performance, model_objs\n",
    "\n",
    "def select_and_save_best_model(performance: dict, model_objs: dict, plant: str, model_dir: Path):\n",
    "    \"\"\"\n",
    "    Selects the best model (highest R²) and saves it to disk using joblib.\n",
    "    The model is saved with a .h5 extension.\n",
    "    \"\"\"\n",
    "    if not performance:\n",
    "        print(f\"No performance metrics for {plant}. Skipping model saving.\")\n",
    "        return None\n",
    "    best_model_name = max(performance, key=lambda m: performance[m][\"R2\"])\n",
    "    best_model = model_objs[best_model_name]\n",
    "    model_file = model_dir / f\"{plant}_best_model.h5\"\n",
    "    joblib.dump(best_model, model_file)\n",
    "    print(f\"Best model for {plant} is {best_model_name} (R2 = {performance[best_model_name]['R2']:.2f}) and has been saved to {model_file}\")\n",
    "    return best_model_name\n",
    "\n",
    "# =============================================\n",
    "# Step 4. Main processing: Load, preprocess, model, and save results\n",
    "# =============================================\n",
    "def main():\n",
    "    # Define paths (adjust as needed)\n",
    "    meteo_dir = Path(\"../data/interim/meteo_data_with_generation\")\n",
    "    output_data_dir = Path(\"../data/interim/meteo_data_with_generation/processed_models\")\n",
    "    output_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_dir = output_data_dir / \"models\"\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load all merged data from Parquet files\n",
    "    plant_dfs = load_all_merged_data(meteo_dir)\n",
    "    \n",
    "    overall_results = {}\n",
    "    best_models = {}\n",
    "    \n",
    "    for plant, df in plant_dfs.items():\n",
    "        print(f\"\\nProcessing plant: {plant}\")\n",
    "        # Step 2a: Drop the initial consecutive days with zero generation\n",
    "        df_clean = drop_initial_zero_generation_days(df)\n",
    "        # Step 2b: Drop the last 26 hours (incomplete data)\n",
    "        df_clean = drop_last_26_hours(df_clean)\n",
    "        # Save cleaned DataFrame (optional)\n",
    "        clean_path = output_data_dir / f\"{plant}_clean.parquet\"\n",
    "        df_clean.to_parquet(clean_path, index=True)\n",
    "        print(f\"Saved cleaned data for {plant} to {clean_path}\")\n",
    "        \n",
    "        # Step 3: Train and evaluate models using meteorological features to predict generation\n",
    "        if not df_clean.empty:\n",
    "            performance, model_objs = tune_and_evaluate_models(df_clean, plant)\n",
    "            overall_results[plant] = performance\n",
    "            best_model_name = select_and_save_best_model(performance, model_objs, plant, model_dir)\n",
    "            best_models[plant] = best_model_name\n",
    "        else:\n",
    "            print(f\"No data remains for {plant} after cleaning.\")\n",
    "    \n",
    "    # Save overall performance metrics to a CSV file for later review\n",
    "    results_df = pd.DataFrame.from_dict({plant: metrics for plant, metrics in overall_results.items()}, orient=\"index\")\n",
    "    results_csv = output_data_dir / \"model_performance_results.csv\"\n",
    "    results_df.to_csv(results_csv)\n",
    "    print(\"\\nOverall model performance results:\")\n",
    "    print(results_df)\n",
    "    print(f\"Saved performance metrics to {results_csv}\")\n",
    "\n",
    "# Execute the main function (suitable for a Jupyter Notebook)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
