{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and Merge Generation Data from .parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged 4461 DataFrames.\n",
      "The resulting DataFrame has 476594 rows and 30 columns.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "# Folder containing the .parquet files\n",
    "folder_path = r\"..\\data\\raw\\post_despacho_data\"\n",
    "\n",
    "# Find all .parquet files in the folder\n",
    "parquet_files = glob.glob(os.path.join(folder_path, \"*.parquet\"))\n",
    "\n",
    "if not parquet_files:\n",
    "    print(\"No Parquet files were found in the specified folder.\")\n",
    "else:\n",
    "    # Load each Parquet file and add it to a list\n",
    "    df_list = []\n",
    "    for file in parquet_files:\n",
    "        #print(f\"Importing {file}...\")\n",
    "        df_temp = pd.read_parquet(file)\n",
    "        df_list.append(df_temp)\n",
    "\n",
    "    # Concatenate all DataFrames into a single one\n",
    "    df_merged = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"\\nMerged {len(df_list)} DataFrames.\")\n",
    "    print(f\"The resulting DataFrame has {df_merged.shape[0]} rows and {df_merged.shape[1]} columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normilize_string and Date Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Functions ---\n",
    "\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_string(s):\n",
    "    \"\"\"\n",
    "    Converts a string to lowercase and removes accents.\n",
    "    \n",
    "    Parameters:\n",
    "        s (str): The input string.\n",
    "    \n",
    "    Returns:\n",
    "        str: The normalized string.\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    s = ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n",
    "    return s\n",
    "\n",
    "def normalize_central_column(df, column=\"CENTRAL\"):\n",
    "    \"\"\"\n",
    "    Normalizes the values in the specified column by converting to lowercase and removing accents.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the column.\n",
    "        column (str): Column name to normalize (default \"CENTRAL\").\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame.\n",
    "    \"\"\"\n",
    "    df[column] = df[column].astype(str).apply(normalize_string)\n",
    "    return df\n",
    "\n",
    "def format_fecha_column(df, column=\"FECHA\"):\n",
    "    \"\"\"\n",
    "    Converts the specified date column to datetime using '%Y-%m-%dT%H:%M:%S'.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the date column.\n",
    "        column (str): Column name to format (default \"FECHA\").\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame.\n",
    "    \"\"\"\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%dT%H:%M:%S', errors='coerce')\n",
    "    return df\n",
    "\n",
    "def remove_unwanted_units(df, unit_col=\"CENTRAL\"):\n",
    "    \"\"\"\n",
    "    Removes rows from the DataFrame where the unit (in unit_col) is in the exclude list.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with the unit column.\n",
    "        unit_col (str): Name of the unit column (default \"CENTRAL\").\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with unwanted units removed.\n",
    "    \"\"\"\n",
    "    exclude_units = [\n",
    "        \"arroyo barril\",\n",
    "        \"cayman\",\n",
    "        \"dajabon\",\n",
    "        \"los mina 1\",\n",
    "        \"los mina 2\",\n",
    "        \"haina 3\",\n",
    "        \"puerto plata 1\",\n",
    "        \"puerto plata 2\",\n",
    "        \"santo domingo 5\",\n",
    "        \"santo domingo 8\",\n",
    "        \"timbeque 1\",\n",
    "        \"timbeque 2\",\n",
    "        \"san pedro vapor\"\n",
    "    ]\n",
    "    df = df[~df[unit_col].isin(exclude_units)]\n",
    "    return df\n",
    "\n",
    "def fill_missing_with_next_available(df, date_col='FECHA', central_col='CENTRAL', max_offset=14):\n",
    "    \"\"\"\n",
    "    Fills missing dates in the DataFrame by finding records from a subsequent date that \n",
    "    has the same weekday as the missing date. For each missing date, the function checks\n",
    "    offsets starting at 7 days up to max_offset days. If a record is found, it is duplicated \n",
    "    and its date is set to the missing date.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with at least the date_col and central_col.\n",
    "        date_col (str): Name of the date column (assumed to be datetime).\n",
    "        central_col (str): Name of the plant column.\n",
    "        max_offset (int): Maximum number of days offset to check (default is 14).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with missing dates filled.\n",
    "    \"\"\"\n",
    "    df[date_col] = pd.to_datetime(df[date_col]).dt.normalize()\n",
    "    all_dates = pd.date_range(start=df[date_col].min(), end=df[date_col].max())\n",
    "    present_dates = pd.to_datetime(df[date_col].unique())\n",
    "    missing_dates = all_dates.difference(present_dates)\n",
    "    \n",
    "    fill_rows = []\n",
    "    \n",
    "    for missing_date in missing_dates:\n",
    "        filled = False\n",
    "        for offset in range(7, max_offset + 1):\n",
    "            candidate_date = missing_date + pd.Timedelta(days=offset)\n",
    "            if candidate_date.weekday() != missing_date.weekday():\n",
    "                continue\n",
    "            df_candidate = df[df[date_col] == candidate_date]\n",
    "            if not df_candidate.empty:\n",
    "                for _, row in df_candidate.iterrows():\n",
    "                    new_row = row.copy()\n",
    "                    new_row[date_col] = missing_date\n",
    "                    fill_rows.append(new_row)\n",
    "                filled = True\n",
    "                break\n",
    "        if not filled:\n",
    "            print(f\"No matching record found to fill missing date {missing_date.date()}\")\n",
    "    \n",
    "    if fill_rows:\n",
    "        df_filled = pd.concat([df, pd.DataFrame(fill_rows)], ignore_index=True)\n",
    "    else:\n",
    "        df_filled = df.copy()\n",
    "    \n",
    "    df_filled = df_filled.sort_values(by=[central_col, date_col]).reset_index(drop=True)\n",
    "    return df_filled\n",
    "def standardize_central_names(df, column=\"CENTRAL\"):\n",
    "    \"\"\"\n",
    "    Standardizes central names by applying a mapping. This function assumes that the values \n",
    "    in the specified column are already in lowercase.\n",
    "    \n",
    "    Mappings applied:\n",
    "      - \"central hidroelectrica hatillo 2\" -> \"hatillo 2\"\n",
    "      - \"parque fotovoltaico bayahonda (bayasol)\" -> \"parque fotovoltaico bayasol\"\n",
    "      - \"parque fotovoltaico montecristi solar1\" -> \"parque fotovoltaico montecristi solar 1\"\n",
    "      - \"parque eolico los guzmancito 2\" -> \"parque eolico los guzmancitos 2\"\n",
    "      - \"hatillo\" -> \"hatillo 1\"\n",
    "      - \"juancho los cocos 1\" -> \"los cocos 1\"\n",
    "      - \"aes andres xxxxxx\" -> \"aes andres\"\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the central names.\n",
    "        column (str): Name of the column to standardize (default \"CENTRAL\").\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with standardized central names.\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"central hidroelectrica hatillo 2\": \"hatillo 2\",\n",
    "        \"parque fotovoltaico bayahonda (bayasol)\": \"parque fotovoltaico bayasol\",\n",
    "        \"parque fotovoltaico montecristi solar1\": \"parque fotovoltaico montecristi solar 1\",\n",
    "        \"parque eolico los guzmancito 2\": \"parque eolico los guzmancitos 2\",\n",
    "        \"hatillo\": \"hatillo 1\",\n",
    "        \"juancho los cocos 1\": \"los cocos 1\",\n",
    "        \"aes andres xxxxxx\": \"aes andres\"\n",
    "    }\n",
    "    \n",
    "    df[column] = df[column].replace(mapping)\n",
    "    return df\n",
    "def remove_daily_duplicates_by_max_hours(df, date_col=\"FECHA\", unit_col=\"CENTRAL\"):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows for the same unit (central) and date by keeping only the row \n",
    "    with the highest sum of hour columns (H1 to H24).\n",
    "    \n",
    "    Assumes the date column is already normalized (i.e., time set to midnight).\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing at least the columns specified in date_col, \n",
    "                           unit_col, and hour columns (H1...H24).\n",
    "        date_col (str): Name of the date column (default \"FECHA\").\n",
    "        unit_col (str): Name of the central column (default \"CENTRAL\").\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with duplicates removed, keeping only the row with the maximum \n",
    "                      hour sum for each combination of unit and date.\n",
    "    \"\"\"\n",
    "    # Identify hour columns (H1 to H24) present in the DataFrame\n",
    "    hour_cols = [f\"H{i}\" for i in range(1, 25) if f\"H{i}\" in df.columns]\n",
    "    if not hour_cols:\n",
    "        print(\"No hour columns found in the DataFrame.\")\n",
    "        return df\n",
    "    \n",
    "    # Calculate the sum of hour columns for each row and add it as a temporary column\n",
    "    df[\"hour_sum\"] = df[hour_cols].sum(axis=1, skipna=True)\n",
    "    \n",
    "    # Group by date and unit, and select the index of the row with the maximum hour_sum for each group\n",
    "    idx = df.groupby([date_col, unit_col])[\"hour_sum\"].idxmax()\n",
    "    \n",
    "    # Retrieve those rows and remove the temporary hour_sum column\n",
    "    df_unique = df.loc[idx].copy()\n",
    "    df_unique.drop(columns=[\"hour_sum\"], inplace=True)\n",
    "    \n",
    "    return df_unique\n",
    "\n",
    "def aggregate_unit_groups(df, group_mapping, date_col=\"FECHA\", unit_col=\"CENTRAL\"):\n",
    "    \"\"\"\n",
    "    Aggregates rows for unit groups based on a mapping.\n",
    "    \n",
    "    For each group defined in group_mapping (where the key is the unified unit name and the value\n",
    "    is a list of variants), the function:\n",
    "      1. Filters rows where unit_col is in the variants.\n",
    "      2. Groups these rows by the date (date_col) and sums the hourly columns (H1 to H24).\n",
    "      3. Creates new rows with the unified unit name (the key).\n",
    "      4. Removes the original rows for these variants from the DataFrame.\n",
    "      5. Appends the aggregated rows back to the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame containing at least the date_col, unit_col, and hourly columns (H1 ... H24).\n",
    "      group_mapping (dict): Dictionary where keys are the unified unit names and values are lists of variants.\n",
    "      date_col (str): Name of the date column (default \"FECHA\").\n",
    "      unit_col (str): Name of the unit column (default \"CENTRAL\").\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: DataFrame with aggregated rows for each specified group.\n",
    "    \"\"\"\n",
    "    # Identify hourly columns (H1 to H24) that exist in the DataFrame\n",
    "    hour_cols = [f\"H{i}\" for i in range(1, 25) if f\"H{i}\" in df.columns]\n",
    "    if not hour_cols:\n",
    "        print(\"No hourly columns found in the DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    # Normalize the date column (set time to midnight)\n",
    "    df[date_col] = pd.to_datetime(df[date_col]).dt.normalize()\n",
    "    \n",
    "    aggregated_rows = []\n",
    "    \n",
    "    # Loop over each group in the mapping\n",
    "    for unified_unit, variants in group_mapping.items():\n",
    "        # Filter rows where unit is in the variants list\n",
    "        mask = df[unit_col].isin(variants)\n",
    "        df_group = df[mask].copy()\n",
    "        if df_group.empty:\n",
    "            continue\n",
    "        # Group by date and sum the hourly columns\n",
    "        df_agg = df_group.groupby(date_col, as_index=False)[hour_cols].sum()\n",
    "        # Set the unit column to the unified unit for all aggregated rows\n",
    "        df_agg[unit_col] = unified_unit\n",
    "        \n",
    "        aggregated_rows.append(df_agg)\n",
    "    \n",
    "    # If any aggregated rows were created, combine them\n",
    "    if aggregated_rows:\n",
    "        df_aggregated = pd.concat(aggregated_rows, ignore_index=True)\n",
    "    else:\n",
    "        df_aggregated = pd.DataFrame(columns=[date_col, unit_col] + hour_cols)\n",
    "    \n",
    "    # Remove original rows that belong to any of the variants in the mapping\n",
    "    all_variants = [variant for variants in group_mapping.values() for variant in variants]\n",
    "    df_remaining = df[~df[unit_col].isin(all_variants)]\n",
    "    \n",
    "    # Combine the remaining rows with the aggregated rows\n",
    "    df_result = pd.concat([df_remaining, df_aggregated], ignore_index=True)\n",
    "    \n",
    "    # Sort by unit and date for clarity\n",
    "    df_result = df_result.sort_values(by=[unit_col, date_col]).reset_index(drop=True)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "def aggregate_los_mina(df, date_col=\"FECHA\", unit_col=\"CENTRAL\"):\n",
    "    \"\"\"\n",
    "    Aggregates rows for 'los mina' by processing two groups:\n",
    "      - Group 1: Sum hourly values from rows with unit in [\"los mina 5\", \"los mina 6\", \"los mina 7\"].\n",
    "      - Group 2: For rows with unit in [\"parque energetico los mina cc parcial\", \"parque energetico los mina cc total\"],\n",
    "                 select the row with the maximum hour sum (H1 to H24) for each day.\n",
    "    \n",
    "    For each day with data in either group, a new aggregated row with unit \"los mina\" is created,\n",
    "    with hourly values equal to the sum of Group 1 and Group 2 values.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame containing at least the date_col, unit_col, and hourly columns (H1 ... H24).\n",
    "      date_col (str): Name of the date column (assumed to be datetime or will be normalized).\n",
    "      unit_col (str): Name of the unit/central column.\n",
    "      \n",
    "    Returns:\n",
    "      pd.DataFrame: DataFrame with the original rows for these groups removed and replaced with aggregated rows.\n",
    "    \"\"\"\n",
    "    # Ensure the date column is datetime and normalized (time set to midnight)\n",
    "    df[date_col] = pd.to_datetime(df[date_col]).dt.normalize()\n",
    "    \n",
    "    # Determine hourly columns (H1 to H24) available in df\n",
    "    hour_cols = [f\"H{i}\" for i in range(1, 25) if f\"H{i}\" in df.columns]\n",
    "    if not hour_cols:\n",
    "        print(\"No hourly columns found.\")\n",
    "        return df\n",
    "    \n",
    "    # Define the variant groups\n",
    "    group1_variants = [\"los mina 5\", \"los mina 6\", \"los mina 7\"]\n",
    "    group2_variants = [\"parque energetico los mina cc parcial\", \"parque energetico los mina cc total\"]\n",
    "    \n",
    "    # Filter rows for each group\n",
    "    df_group1 = df[df[unit_col].isin(group1_variants)].copy()\n",
    "    df_group2 = df[df[unit_col].isin(group2_variants)].copy()\n",
    "    \n",
    "    # Determine all unique dates where either group has data\n",
    "    dates = pd.to_datetime(pd.concat([df_group1[date_col], df_group2[date_col]]).unique())\n",
    "    \n",
    "    aggregated_rows = []\n",
    "    \n",
    "    for d in dates:\n",
    "        # Initialize aggregated hourly values for the day as zeros\n",
    "        agg_values = {col: 0 for col in hour_cols}\n",
    "        \n",
    "        # Group 1 aggregation: Sum rows for the date d if present\n",
    "        df1_d = df_group1[df_group1[date_col] == d]\n",
    "        if not df1_d.empty:\n",
    "            sum_group1 = df1_d[hour_cols].sum()\n",
    "            for col in hour_cols:\n",
    "                agg_values[col] += sum_group1[col]\n",
    "        \n",
    "        # Group 2 aggregation: For the date d, select the row with maximum hour sum if present\n",
    "        df2_d = df_group2[df_group2[date_col] == d].copy()  # <-- Make an explicit copy here\n",
    "        if not df2_d.empty:\n",
    "            df2_d[\"hour_sum\"] = df2_d[hour_cols].sum(axis=1, skipna=True)\n",
    "            idx = df2_d[\"hour_sum\"].idxmax()\n",
    "            max_row = df2_d.loc[idx]\n",
    "            for col in hour_cols:\n",
    "                agg_values[col] += max_row[col]\n",
    "        \n",
    "        # Create the aggregated row if there was data in either group for the day\n",
    "        if not (df1_d.empty and df_group2.empty):\n",
    "            new_row = {date_col: d, unit_col: \"los mina\"}\n",
    "            for col in hour_cols:\n",
    "                new_row[col] = agg_values[col]\n",
    "            aggregated_rows.append(new_row)\n",
    "    \n",
    "    # Create a DataFrame from aggregated rows\n",
    "    if aggregated_rows:\n",
    "        df_aggregated = pd.DataFrame(aggregated_rows)\n",
    "    else:\n",
    "        df_aggregated = pd.DataFrame(columns=[date_col, unit_col] + hour_cols)\n",
    "    \n",
    "    # Remove original rows for both groups from the DataFrame\n",
    "    df_remaining = df[~df[unit_col].isin(group1_variants + group2_variants)]\n",
    "    \n",
    "    # Combine the remaining rows with the new aggregated rows\n",
    "    df_result = pd.concat([df_remaining, df_aggregated], ignore_index=True)\n",
    "    \n",
    "    # Sort by unit and date\n",
    "    df_result = df_result.sort_values(by=[unit_col, date_col]).reset_index(drop=True)\n",
    "    \n",
    "    return df_result\n",
    "def DB_post_despacho_transformer(df):\n",
    "    import pandas as pd\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Drop unwanted columns\n",
    "    cols_to_drop = [\"GRUPOS\", \"INDICE\", \"GRUPO\", \"EMPRESA\"]\n",
    "    df = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    # Identify hour columns (H1..H24)\n",
    "    hour_cols = [col for col in df.columns if col.startswith(\"H\")]\n",
    "    \n",
    "    # 2. Melt the DataFrame\n",
    "    df_melted = df.melt(\n",
    "        id_vars=[\"CENTRAL\", \"FECHA\"], \n",
    "        value_vars=hour_cols,\n",
    "        var_name=\"Hour\", \n",
    "        value_name=\"Generation\"\n",
    "    )\n",
    "    \n",
    "    # Ensure FECHA is datetime\n",
    "    df_melted[\"FECHA\"] = pd.to_datetime(df_melted[\"FECHA\"], errors=\"coerce\")\n",
    "    \n",
    "    # 3. Convert 'Hour' -> integer, subtract 1 so H1 = 0:00\n",
    "    df_melted[\"HourNum\"] = df_melted[\"Hour\"].str.lstrip(\"H\").astype(int) - 1\n",
    "    \n",
    "    # 4. Create a real datetime 'timestamp' by adding hours\n",
    "    # Use unit=\"h\" instead of \"H\"\n",
    "    df_melted[\"timestamp\"] = df_melted[\"FECHA\"] + pd.to_timedelta(df_melted[\"HourNum\"], unit=\"h\")\n",
    "    \n",
    "    # 5. Pivot so that each CENTRAL is a column, indexed by timestamp\n",
    "    df_pivot = df_melted.pivot_table(\n",
    "        index=\"timestamp\", \n",
    "        columns=\"CENTRAL\", \n",
    "        values=\"Generation\", \n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    \n",
    "    # Reset index so 'timestamp' is a normal column\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "\n",
    "    return df_pivot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed DataFrame (timestamp and plant columns):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>CENTRAL</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>aes andres</th>\n",
       "      <th>aguacate 1</th>\n",
       "      <th>aguacate 2</th>\n",
       "      <th>aniana vargas 1</th>\n",
       "      <th>aniana vargas 2</th>\n",
       "      <th>baiguaque 1</th>\n",
       "      <th>baiguaque 2</th>\n",
       "      <th>barahona carbon</th>\n",
       "      <th>bersal</th>\n",
       "      <th>...</th>\n",
       "      <th>tavera 1</th>\n",
       "      <th>tavera 2</th>\n",
       "      <th>total eolico</th>\n",
       "      <th>total generado</th>\n",
       "      <th>total hidroelectrica</th>\n",
       "      <th>total programado</th>\n",
       "      <th>total solar</th>\n",
       "      <th>total termico</th>\n",
       "      <th>valdesia 1</th>\n",
       "      <th>valdesia 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>207.00</td>\n",
       "      <td>25.7</td>\n",
       "      <td>26.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.66</td>\n",
       "      <td>1557.26</td>\n",
       "      <td>167.33</td>\n",
       "      <td>1695.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1378.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 01:00:00</td>\n",
       "      <td>241.00</td>\n",
       "      <td>25.7</td>\n",
       "      <td>25.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.60</td>\n",
       "      <td>1534.19</td>\n",
       "      <td>165.33</td>\n",
       "      <td>1627.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1348.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 02:00:00</td>\n",
       "      <td>217.00</td>\n",
       "      <td>25.7</td>\n",
       "      <td>25.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.88</td>\n",
       "      <td>1453.91</td>\n",
       "      <td>170.33</td>\n",
       "      <td>1546.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1278.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 03:00:00</td>\n",
       "      <td>220.00</td>\n",
       "      <td>25.7</td>\n",
       "      <td>25.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1407.16</td>\n",
       "      <td>170.33</td>\n",
       "      <td>1467.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1236.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 04:00:00</td>\n",
       "      <td>225.00</td>\n",
       "      <td>25.7</td>\n",
       "      <td>25.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1382.51</td>\n",
       "      <td>170.33</td>\n",
       "      <td>1419.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1211.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107059</th>\n",
       "      <td>2025-03-19 19:00:00</td>\n",
       "      <td>266.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>48.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>32.70</td>\n",
       "      <td>43.21</td>\n",
       "      <td>27.38</td>\n",
       "      <td>3142.09</td>\n",
       "      <td>326.39</td>\n",
       "      <td>3221.14</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2788.27</td>\n",
       "      <td>21.70</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107060</th>\n",
       "      <td>2025-03-19 20:00:00</td>\n",
       "      <td>254.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>48.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>33.41</td>\n",
       "      <td>42.68</td>\n",
       "      <td>20.02</td>\n",
       "      <td>3147.54</td>\n",
       "      <td>331.17</td>\n",
       "      <td>3273.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2796.35</td>\n",
       "      <td>21.76</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107061</th>\n",
       "      <td>2025-03-19 21:00:00</td>\n",
       "      <td>244.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>49.22</td>\n",
       "      <td>2.88</td>\n",
       "      <td>...</td>\n",
       "      <td>33.26</td>\n",
       "      <td>42.35</td>\n",
       "      <td>20.00</td>\n",
       "      <td>3102.88</td>\n",
       "      <td>331.70</td>\n",
       "      <td>3257.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2751.18</td>\n",
       "      <td>21.75</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107062</th>\n",
       "      <td>2025-03-19 22:00:00</td>\n",
       "      <td>253.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>47.96</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>32.84</td>\n",
       "      <td>42.06</td>\n",
       "      <td>11.76</td>\n",
       "      <td>3044.96</td>\n",
       "      <td>308.64</td>\n",
       "      <td>3223.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2724.56</td>\n",
       "      <td>21.74</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107063</th>\n",
       "      <td>2025-03-19 23:00:00</td>\n",
       "      <td>271.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>48.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.57</td>\n",
       "      <td>43.84</td>\n",
       "      <td>12.38</td>\n",
       "      <td>2978.28</td>\n",
       "      <td>268.41</td>\n",
       "      <td>3123.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2697.49</td>\n",
       "      <td>21.73</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107064 rows Ã— 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "CENTRAL           timestamp  aes andres  aguacate 1  aguacate 2  \\\n",
       "0       2013-01-01 00:00:00      207.00        25.7       26.70   \n",
       "1       2013-01-01 01:00:00      241.00        25.7       25.70   \n",
       "2       2013-01-01 02:00:00      217.00        25.7       25.70   \n",
       "3       2013-01-01 03:00:00      220.00        25.7       25.70   \n",
       "4       2013-01-01 04:00:00      225.00        25.7       25.70   \n",
       "...                     ...         ...         ...         ...   \n",
       "107059  2025-03-19 19:00:00      266.11         0.0       26.84   \n",
       "107060  2025-03-19 20:00:00      254.39         0.0       26.61   \n",
       "107061  2025-03-19 21:00:00      244.93         0.0       26.41   \n",
       "107062  2025-03-19 22:00:00      253.63         0.0       26.35   \n",
       "107063  2025-03-19 23:00:00      271.50         0.0       26.39   \n",
       "\n",
       "CENTRAL  aniana vargas 1  aniana vargas 2  baiguaque 1  baiguaque 2  \\\n",
       "0                    0.0              0.0          0.2          0.0   \n",
       "1                    0.0              0.0          0.2          0.0   \n",
       "2                    0.0              0.0          0.2          0.0   \n",
       "3                    0.0              0.0          0.2          0.0   \n",
       "4                    0.0              0.0          0.2          0.0   \n",
       "...                  ...              ...          ...          ...   \n",
       "107059               0.0              0.2          0.0          0.2   \n",
       "107060               0.0              0.2          0.0          0.2   \n",
       "107061               0.0              0.2          0.0          0.2   \n",
       "107062               0.0              0.2          0.0          0.2   \n",
       "107063               0.0              0.2          0.2          0.2   \n",
       "\n",
       "CENTRAL  barahona carbon  bersal  ...  tavera 1  tavera 2  total eolico  \\\n",
       "0                  43.70     NaN  ...      0.00      0.00         11.66   \n",
       "1                  43.50     NaN  ...      0.00      0.00         20.60   \n",
       "2                  44.10     NaN  ...      0.00      0.00          4.88   \n",
       "3                  43.60     NaN  ...      0.00      0.00          0.37   \n",
       "4                  44.20     NaN  ...      0.00      0.00          0.83   \n",
       "...                  ...     ...  ...       ...       ...           ...   \n",
       "107059             48.17    0.00  ...     32.70     43.21         27.38   \n",
       "107060             48.54    0.00  ...     33.41     42.68         20.02   \n",
       "107061             49.22    2.88  ...     33.26     42.35         20.00   \n",
       "107062             47.96    0.33  ...     32.84     42.06         11.76   \n",
       "107063             48.89    0.00  ...      0.57     43.84         12.38   \n",
       "\n",
       "CENTRAL  total generado  total hidroelectrica  total programado  total solar  \\\n",
       "0               1557.26                167.33           1695.10          NaN   \n",
       "1               1534.19                165.33           1627.80          NaN   \n",
       "2               1453.91                170.33           1546.70          NaN   \n",
       "3               1407.16                170.33           1467.40          NaN   \n",
       "4               1382.51                170.33           1419.00          NaN   \n",
       "...                 ...                   ...               ...          ...   \n",
       "107059          3142.09                326.39           3221.14         0.05   \n",
       "107060          3147.54                331.17           3273.15         0.00   \n",
       "107061          3102.88                331.70           3257.00         0.00   \n",
       "107062          3044.96                308.64           3223.82         0.00   \n",
       "107063          2978.28                268.41           3123.23         0.00   \n",
       "\n",
       "CENTRAL  total termico  valdesia 1  valdesia 2  \n",
       "0              1378.27        0.00         0.0  \n",
       "1              1348.26        0.00         0.0  \n",
       "2              1278.70        0.00         0.0  \n",
       "3              1236.46        0.00         0.0  \n",
       "4              1211.35        0.00         0.0  \n",
       "...                ...         ...         ...  \n",
       "107059         2788.27       21.70         0.0  \n",
       "107060         2796.35       21.76         0.0  \n",
       "107061         2751.18       21.75         0.0  \n",
       "107062         2724.56       21.74         0.0  \n",
       "107063         2697.49       21.73         0.0  \n",
       "\n",
       "[107064 rows x 126 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Normalize the 'CENTRAL' column\n",
    "df_merged = normalize_central_column(df_merged)\n",
    "\n",
    "# 2. Format the 'FECHA' column to datetime\n",
    "df_merged = format_fecha_column(df_merged)\n",
    "\n",
    "# 3. Remove unwanted units\n",
    "df_merged = remove_unwanted_units(df_merged, unit_col=\"CENTRAL\")\n",
    "\n",
    "# 4. Fill missing dates by copying records from the same weekday one week later\n",
    "df_merged = fill_missing_with_next_available(df_merged, date_col='FECHA', central_col='CENTRAL')\n",
    "\n",
    "# 5. Standardize central names\n",
    "df_merged = standardize_central_names(df_merged, column=\"CENTRAL\")\n",
    "\n",
    "# 6. Remove daily duplicates by keeping the row with the highest hour sum\n",
    "df_merged = remove_daily_duplicates_by_max_hours(df_merged, date_col=\"FECHA\", unit_col=\"CENTRAL\")\n",
    "\n",
    "group_mapping = {\n",
    "    \"aes andres\": [\"aes andres fo\", \"aes andres gn\", \"aes andres\"],\n",
    "    \"cespm 1\": [\"cespm 1 fo\", \"cespm 1 gn\", \"cespm 1\"],\n",
    "    \"cespm 2\": [\"cespm 2 fo\", \"cespm 2 gn\", \"cespm 2\"],\n",
    "    \"cespm 3\": [\"cespm 3 fo\", \"cespm 3 gn\", \"cespm 3\"],\n",
    "    \"estrella del mar 2\": [\"estrella del mar 2 cfo\", \"estrella del mar 2 cgn\", \"estrella del mar 2 sfo\", \"estrella del mar 2 sgn\"],\n",
    "    \"estrella del mar 3\": [\"estrella del mar 3 ccp\", \"estrella del mar 3 cct\", \"estrella del mar 3 cs\", \"estrella del mar 3 sgn\", \"estrella del mar 3\"],\n",
    "    \"powership azua\": [\"powership azua kps 26\", \"powership azua kps 60\", \"powership azua\"],\n",
    "    \"los origenes\": [\"los origenes power plant fuel oil\", \"los origenes power plant gas natural\", \"los origenes\"],\n",
    "    \"quisqueya 1\": [\"quisqueya 1 fo\", \"quisqueya 1 gn\", \"quisqueya 1 san pedro\", \"quisqueya 1 san pedro fo\", \n",
    "                     \"quisqueya 1 san pedro gn\", \"quisqueya 1b san pedro\", \"quisqueya 1b san pedro fo\", \n",
    "                     \"quisqueya 1b san pedro gn\", \"quisqueya 1\"],\n",
    "    \"quisqueya 2\": [\"quisqueya 2 fo\", \"quisqueya 2 gn\"],\n",
    "    \"san felipe\": [\"san felipe cc\", \"san felipe vap\", \"san felipe\"],\n",
    "    \"los cocos 1\": [\"juancho los cocos 1\", \"los cocos 1\"],\n",
    "    \"parque eolico de matafongo\": ['parque eolico de matafongo', 'parque eolico matafongo']\n",
    "}\n",
    "# 7. aggregate \n",
    "df_merged = aggregate_unit_groups(df_merged, group_mapping, date_col=\"FECHA\", unit_col=\"CENTRAL\")\n",
    "df_merged = aggregate_los_mina(df_merged, date_col=\"FECHA\", unit_col=\"CENTRAL\")\n",
    "\n",
    "# 8.Pivot\n",
    "df_transformed = DB_post_despacho_transformer(df_merged)\n",
    "\n",
    "# Display the first few rows of the transformed DataFrame\n",
    "print(\"Transformed DataFrame (timestamp and plant columns):\")\n",
    "df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timestamp', 'aes andres', 'aguacate 1', 'aguacate 2', 'aniana vargas 1', 'aniana vargas 2', 'baiguaque 1', 'baiguaque 2', 'barahona carbon', 'bersal', 'brazo derecho', 'cepp 1', 'cepp 2', 'cespm 1', 'cespm 2', 'cespm 3', 'consumo', 'contra embalse moncion 1', 'contra embalse moncion 2', 'disponibilidad real', 'disponibilidad real sinc.', 'domingo rodriguez 1', 'domingo rodriguez 2', 'el salto', 'estrella del mar', 'estrella del mar 2', 'estrella del mar 3', 'generacion de emergencia aes andres', 'haina 1', 'haina 2', 'haina 4', 'haina tg', 'hatillo 1', 'hatillo 2', 'inca km22', 'itabo 1', 'itabo 2', 'jiguey 1', 'jiguey 2', 'jimenoa', 'la vega', 'las barias', 'las damas', 'lopez angostura', 'los anones', 'los cocos 1', 'los cocos 2', 'los mina', 'los origenes', 'los toros 1', 'los toros 2', 'magueyal 1', 'magueyal 2', 'metaldom', 'moncion 1', 'moncion 2', 'monte plata solar', 'monte rio', 'nizao najayo', 'palamara', 'palenque', 'palomino 1', 'palomino 2', 'parque eolico agua clara', 'parque eolico de matafongo', 'parque eolico guanillo', 'parque eolico larimar', 'parque eolico larimar ii', 'parque eolico los guzmancitos', 'parque eolico los guzmancitos 2', 'parque fotovoltaico bayasol', 'parque fotovoltaico calabaza', 'parque fotovoltaico cumayasa 1', 'parque fotovoltaico cumayasa 2', 'parque fotovoltaico la victoria', 'parque fotovoltaico los negros', 'parque fotovoltaico maranatha fase i', 'parque fotovoltaico mata de palma', 'parque fotovoltaico matrisol', 'parque fotovoltaico mirasol', 'parque fotovoltaico montecristi solar 1', 'parque fotovoltaico sajoma', 'parque fotovoltaico santanasol', 'parque fotovoltaico washington capital 2', 'parque fotovoltaico washington capital 3', 'parque solar canoa', 'parque solar el soco', 'parque solar esperanza', 'parque solar girasol', 'pimentel 1', 'pimentel 2', 'pimentel 3', 'pimentel 4', 'pinalito 1', 'pinalito 2', 'potencia no servida', 'powership azua', 'punta catalina 1', 'punta catalina 2', 'quilvio cabrera', 'quisqueya 1', 'quisqueya 2', 'reserva caliente', 'reserva fria', 'rincon', 'rio blanco 1', 'rio blanco 2', 'rio san juan', 'rosa julia de la cruz', 'sabana yegua', 'sabaneta', 'san felipe', 'san lorenzo 1', 'san pedro bio-energy', 'siba', 'sultana del este', 'tavera 1', 'tavera 2', 'total eolico', 'total generado', 'total hidroelectrica', 'total programado', 'total solar', 'total termico', 'valdesia 1', 'valdesia 2']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Show every column name in one go\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(df_transformed.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: ..\\data\\interim\\post_despacho_transformed.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Folder and filename for the output\n",
    "output_folder = r\"..\\data\\interim\"\n",
    "output_filename = \"post_despacho_transformed.parquet\"\n",
    "\n",
    "# Construct the full path\n",
    "output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "# Save df_merged to Parquet (overwriting if it already exists)\n",
    "df_transformed.to_parquet(output_path, index=False)\n",
    "print(f\"File saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CENTRAL\tFirstAppearance\n",
      "aes andres\t2013-01-01\n",
      "aguacate 1\t2013-01-01\n",
      "aguacate 2\t2013-01-01\n",
      "aniana vargas 1\t2013-01-01\n",
      "aniana vargas 2\t2013-01-01\n",
      "baiguaque 1\t2013-01-01\n",
      "baiguaque 2\t2013-01-01\n",
      "barahona carbon\t2013-01-01\n",
      "bersal\t2014-08-23\n",
      "brazo derecho\t2014-09-05\n",
      "cepp 1\t2013-01-01\n",
      "cepp 2\t2013-01-01\n",
      "cespm 1\t2013-01-01\n",
      "cespm 2\t2013-01-01\n",
      "cespm 3\t2013-01-01\n",
      "consumo\t2013-01-01\n",
      "contra embalse moncion 1\t2013-01-01\n",
      "contra embalse moncion 2\t2013-01-01\n",
      "disponibilidad real\t2013-01-01\n",
      "disponibilidad real sinc.\t2013-01-01\n",
      "domingo rodriguez 1\t2013-01-01\n",
      "domingo rodriguez 2\t2013-01-01\n",
      "el salto\t2013-01-01\n",
      "estrella del mar\t2013-01-01\n",
      "estrella del mar 2\t2013-01-01\n",
      "estrella del mar 3\t2021-09-23\n",
      "generacion de emergencia aes andres\t2018-11-30\n",
      "haina 1\t2013-01-01\n",
      "haina 2\t2013-03-08\n",
      "haina 4\t2013-01-01\n",
      "haina tg\t2013-01-01\n",
      "hatillo 1\t2013-01-01\n",
      "hatillo 2\t2019-04-26\n",
      "inca km22\t2013-01-01\n",
      "itabo 1\t2013-01-01\n",
      "itabo 2\t2013-01-01\n",
      "jiguey 1\t2013-01-01\n",
      "jiguey 2\t2013-01-01\n",
      "jimenoa\t2013-01-01\n",
      "la vega\t2013-01-01\n",
      "las barias\t2013-01-01\n",
      "las damas\t2013-01-01\n",
      "lopez angostura\t2013-01-01\n",
      "los anones\t2013-01-01\n",
      "los cocos 1\t2013-01-01\n",
      "los cocos 2\t2013-01-01\n",
      "los mina\t2013-01-01\n",
      "los origenes\t2013-01-01\n",
      "los toros 1\t2013-01-01\n",
      "los toros 2\t2013-01-01\n",
      "magueyal 1\t2013-01-01\n",
      "magueyal 2\t2013-01-01\n",
      "metaldom\t2013-01-01\n",
      "moncion 1\t2013-01-01\n",
      "moncion 2\t2013-01-01\n",
      "monte plata solar\t2016-06-21\n",
      "monte rio\t2013-01-01\n",
      "nizao najayo\t2013-01-01\n",
      "palamara\t2013-01-01\n",
      "palenque\t2018-03-05\n",
      "palomino 1\t2013-01-01\n",
      "palomino 2\t2013-01-01\n",
      "parque eolico agua clara\t2019-02-22\n",
      "parque eolico de matafongo\t2019-04-26\n",
      "parque eolico guanillo\t2019-04-26\n",
      "parque eolico larimar\t2016-05-07\n",
      "parque eolico larimar ii\t2018-10-16\n",
      "parque eolico los guzmancitos\t2019-04-26\n",
      "parque eolico los guzmancitos 2\t2022-11-04\n",
      "parque fotovoltaico bayasol\t2021-03-12\n",
      "parque fotovoltaico calabaza\t2023-05-26\n",
      "parque fotovoltaico cumayasa 1\t2023-09-05\n",
      "parque fotovoltaico cumayasa 2\t2023-09-05\n",
      "parque fotovoltaico la victoria\t2024-11-22\n",
      "parque fotovoltaico los negros\t2023-10-18\n",
      "parque fotovoltaico maranatha fase i\t2024-06-17\n",
      "parque fotovoltaico mata de palma\t2019-04-26\n",
      "parque fotovoltaico matrisol\t2023-07-27\n",
      "parque fotovoltaico mirasol\t2024-11-27\n",
      "parque fotovoltaico montecristi solar 1\t2018-08-28\n",
      "parque fotovoltaico sajoma\t2024-08-13\n",
      "parque fotovoltaico santanasol\t2022-05-27\n",
      "parque fotovoltaico washington capital 2\t2024-11-29\n",
      "parque fotovoltaico washington capital 3\t2024-11-29\n",
      "parque solar canoa\t2019-04-26\n",
      "parque solar el soco\t2022-07-08\n",
      "parque solar esperanza\t2023-04-17\n",
      "parque solar girasol\t2021-07-09\n",
      "pimentel 1\t2013-01-01\n",
      "pimentel 2\t2013-01-01\n",
      "pimentel 3\t2013-01-01\n",
      "pimentel 4\t2024-05-22\n",
      "pinalito 1\t2013-01-01\n",
      "pinalito 2\t2013-01-01\n",
      "potencia no servida\t2013-01-01\n",
      "powership azua\t2023-05-15\n",
      "punta catalina 1\t2019-02-08\n",
      "punta catalina 2\t2019-04-26\n",
      "quilvio cabrera\t2013-03-23\n",
      "quisqueya 1\t2013-09-02\n",
      "quisqueya 2\t2013-09-02\n",
      "reserva caliente\t2013-01-01\n",
      "reserva fria\t2013-01-01\n",
      "rincon\t2013-01-01\n",
      "rio blanco 1\t2013-01-01\n",
      "rio blanco 2\t2013-01-01\n",
      "rio san juan\t2013-01-01\n",
      "rosa julia de la cruz\t2013-01-01\n",
      "sabana yegua\t2013-01-01\n",
      "sabaneta\t2013-01-01\n",
      "san felipe\t2013-01-01\n",
      "san lorenzo 1\t2013-01-01\n",
      "san pedro bio-energy\t2017-02-11\n",
      "siba\t2023-02-17\n",
      "sultana del este\t2013-01-01\n",
      "tavera 1\t2013-01-01\n",
      "tavera 2\t2013-01-01\n",
      "total eolico\t2013-01-01\n",
      "total generado\t2013-01-01\n",
      "total hidroelectrica\t2013-01-01\n",
      "total programado\t2013-01-01\n",
      "total solar\t2019-04-26\n",
      "total termico\t2013-01-01\n",
      "valdesia 1\t2013-01-01\n",
      "valdesia 2\t2013-01-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Group by \"CENTRAL\" and get the minimum date from the \"FECHA\" column for each group\n",
    "first_appearance = df_merged.groupby(\"CENTRAL\")[\"FECHA\"].min().reset_index()\n",
    "\n",
    "# Rename the FECHA column to \"FirstAppearance\"\n",
    "first_appearance.rename(columns={\"FECHA\": \"FirstAppearance\"}, inplace=True)\n",
    "\n",
    "# Convert the \"FirstAppearance\" datetime to a string in \"YYYY-MM-DD\" format\n",
    "first_appearance[\"FirstAppearance\"] = first_appearance[\"FirstAppearance\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Print the resulting DataFrame as a tab-delimited string\n",
    "print(first_appearance.to_csv(index=False, sep='\\t'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
